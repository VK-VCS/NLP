{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9WJyrBWYxCXTzGa8d+S/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VK-VCS/NLP/blob/main/Encoder_Decoder_with_Attention_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcGlNI9xcyEX",
        "outputId": "def23329-ea72-4fb0-8f85-83da278f133e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch loss: 9.02610969543457\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense\n",
        "import numpy as np\n",
        "\n",
        "# Define hyperparameters\n",
        "BATCH_SIZE = 64\n",
        "EMBEDDING_DIM = 256\n",
        "UNITS = 512\n",
        "VOCAB_SIZE = 10000  # Define based on your dataset\n",
        "MAX_LEN = 50        # Maximum sequence length\n",
        "\n",
        "# Encoder class\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = GRU(enc_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x)\n",
        "        return output, state  # Return the sequence and final hidden state\n",
        "\n",
        "# Attention mechanism\n",
        "class Attention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(Attention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        query_with_time_axis = tf.expand_dims(query, 1)  # Add time axis\n",
        "        score = self.V(tf.nn.tanh(self.W1(values) + self.W2(query_with_time_axis)))\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "        return context_vector, attention_weights\n",
        "\n",
        "# Decoder class\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = GRU(dec_units, return_sequences=True, return_state=True, recurrent_initializer='glorot_uniform')\n",
        "        self.fc = Dense(vocab_size)\n",
        "        self.attention = Attention(dec_units)\n",
        "\n",
        "    def call(self, x, hidden, enc_output):\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "        x = self.embedding(x)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        output, state = self.gru(x)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state, attention_weights\n",
        "\n",
        "# Training step\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.not_equal(real, 0)\n",
        "    loss = loss_object(real, pred)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden, encoder, decoder, targ_lang_tokenizer):\n",
        "    loss = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp)\n",
        "        dec_hidden = enc_hidden\n",
        "        dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = loss / int(targ.shape[1])\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    return batch_loss\n",
        "\n",
        "# Initialize the model\n",
        "encoder = Encoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
        "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, UNITS)\n",
        "\n",
        "# Example input data\n",
        "example_input_batch = tf.random.uniform((BATCH_SIZE, MAX_LEN), minval=1, maxval=VOCAB_SIZE, dtype=tf.int32)\n",
        "example_target_batch = tf.random.uniform((BATCH_SIZE, MAX_LEN), minval=1, maxval=VOCAB_SIZE, dtype=tf.int32)\n",
        "\n",
        "# Tokenizer (mock example for simplicity; replace with actual tokenizer)\n",
        "class MockTokenizer:\n",
        "    def __init__(self, vocab_size):\n",
        "        self.word_index = {f\"word{i}\": i for i in range(1, vocab_size)}\n",
        "        self.word_index[\"<start>\"] = 1\n",
        "        self.word_index[\"<end>\"] = 2\n",
        "\n",
        "targ_lang_tokenizer = MockTokenizer(VOCAB_SIZE)\n",
        "\n",
        "# Train one step\n",
        "sample_hidden = tf.zeros((BATCH_SIZE, UNITS))\n",
        "batch_loss = train_step(example_input_batch, example_target_batch, sample_hidden, encoder, decoder, targ_lang_tokenizer)\n",
        "print(f\"Batch loss: {batch_loss.numpy()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plKoVtxxc_Nn",
        "outputId": "a170885a-139c-43fb-9a1a-0d4af9dcc143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 50), dtype=int32, numpy=\n",
              "array([[7016, 3918, 5876, ..., 3926, 8196, 5857],\n",
              "       [6662, 5179, 1326, ..., 4975, 8471, 3869],\n",
              "       [8777, 5281, 6305, ..., 3312, 4697, 9123],\n",
              "       ...,\n",
              "       [4687, 9361, 4526, ..., 7059, 7347, 9896],\n",
              "       [1665, 2001, 9261, ...,  139, 8026, 9279],\n",
              "       [7482, 5396, 7223, ...,  275, 1239, 2462]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## # Maximum sequence length = 50\n",
        "## mock sentences generated = 64"
      ],
      "metadata": {
        "id": "ZrLjJb_ydnea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_target_batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-9D4FKRdx_n",
        "outputId": "b8c423ee-382d-468f-c0b0-4f0cf63322ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 50), dtype=int32, numpy=\n",
              "array([[2766, 2663, 3783, ..., 1871, 2723, 6020],\n",
              "       [3782, 5800, 9923, ..., 8229, 2056, 3285],\n",
              "       [4578, 9453, 5416, ..., 9048,  210, 6338],\n",
              "       ...,\n",
              "       [5693, 1072, 2105, ...,  658, 8544, 7083],\n",
              "       [5870, 7969, 8796, ..., 7422,  495, 2927],\n",
              "       [6037, 4862, 3418, ..., 1906, 5308, 7958]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(sentence, encoder, decoder, inp_lang_tokenizer, targ_lang_tokenizer, max_length_input, max_length_target):\n",
        "    \"\"\"\n",
        "    Translate a given input sentence using the trained encoder-decoder model.\n",
        "\n",
        "    Args:\n",
        "    - sentence: The input sentence to translate.\n",
        "    - encoder: Trained encoder model.\n",
        "    - decoder: Trained decoder model.\n",
        "    - inp_lang_tokenizer: Tokenizer for the input language.\n",
        "    - targ_lang_tokenizer: Tokenizer for the target language.\n",
        "    - max_length_input: Maximum length of the input sequence.\n",
        "    - max_length_target: Maximum length of the target sequence.\n",
        "\n",
        "    Returns:\n",
        "    - Translation as a string.\n",
        "    - Attention weights for visualization.\n",
        "    \"\"\"\n",
        "    # Preprocess the input sentence\n",
        "    inputs = [inp_lang_tokenizer.word_index.get(word, 0) for word in sentence.split()]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_input, padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "    # Initialize the encoder\n",
        "    enc_out, enc_hidden = encoder(inputs)\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    # Start decoding with the '<start>' token\n",
        "    dec_input = tf.expand_dims([targ_lang_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "    result = \"\"\n",
        "    attention_weights = []\n",
        "\n",
        "    for t in range(max_length_target):\n",
        "        predictions, dec_hidden, attention_weight = decoder(dec_input, dec_hidden, enc_out)\n",
        "\n",
        "        # Save attention weights for visualization\n",
        "        attention_weights.append(attention_weight)\n",
        "\n",
        "        # Get the token with the highest probability\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        # Stop if the '<end>' token is predicted\n",
        "        if targ_lang_tokenizer.index_word[predicted_id] == '<end>':\n",
        "            break\n",
        "\n",
        "        # Append the predicted word to the result\n",
        "        result += targ_lang_tokenizer.index_word[predicted_id] + \" \"\n",
        "\n",
        "        # Use the predicted word as the next decoder input\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result.strip(), attention_weights\n",
        "\n",
        "# Translation example\n",
        "def translate(sentence, encoder, decoder, inp_lang_tokenizer, targ_lang_tokenizer, max_length_input, max_length_target):\n",
        "    \"\"\"\n",
        "    Wrapper function for evaluation and displaying translation.\n",
        "    \"\"\"\n",
        "    result, attention_weights = evaluate(sentence, encoder, decoder, inp_lang_tokenizer, targ_lang_tokenizer, max_length_input, max_length_target)\n",
        "    print(f\"Input: {sentence}\")\n",
        "    print(f\"Predicted translation: {result}\")\n",
        "\n",
        "# Example tokenizers\n",
        "class MockTokenizer:\n",
        "    def __init__(self, vocab_size):\n",
        "        self.word_index = {f\"word{i}\": i for i in range(1, vocab_size)}\n",
        "        self.word_index[\"<start>\"] = 1\n",
        "        self.word_index[\"<end>\"] = 2\n",
        "        self.index_word = {i: f\"word{i}\" for i in range(1, vocab_size)}\n",
        "        self.index_word[1] = \"<start>\"\n",
        "        self.index_word[2] = \"<end>\"\n",
        "\n",
        "# Example input tokenizer and target tokenizer\n",
        "inp_lang_tokenizer = MockTokenizer(VOCAB_SIZE)\n",
        "targ_lang_tokenizer = MockTokenizer(VOCAB_SIZE)\n",
        "\n",
        "# Parameters\n",
        "MAX_LEN_INPUT = 50\n",
        "MAX_LEN_TARGET = 50\n",
        "\n",
        "# Translate a sample sentence\n",
        "sample_sentence = \"word10 word20 word30\"\n",
        "translate(sample_sentence, encoder, decoder, inp_lang_tokenizer, targ_lang_tokenizer, MAX_LEN_INPUT, MAX_LEN_TARGET)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDKUE8Oud1Kf",
        "outputId": "ddc8c2d6-873e-4f33-bf1e-f3b0dbc5d7d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: word10 word20 word30\n",
            "Predicted translation: word2315 word5689 word8276 word6753 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867 word7121 word6788 word2867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "svkqVWY2eDUf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}